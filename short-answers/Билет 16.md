Числовые ряды. Необходимый признак сходимости ряда. Признаки сравнения рядов. Признак Даламбера. Признак Коши. Интегральный признак сходимости.
Проверка гипотез о числовых значениях параметров. Проверка гипотез о законе распределения. Проверка гипотез об однородности выборок. Другие задачи проверки статистических гипотез и соответствующие статистические критерии.

---

## Числовые ряды

### Определение числового ряда

**Числовой ряд** — выражение вида:
$$ \sum_{n=1}^{\infty} a_n $$
где $\{a_n\}$ — последовательность действительных или комплексных чисел.

**Частичная сумма ряда** — это сумма первых $n$ членов:
$$ S_n = \sum_{k=1}^{n} a_k $$

**Сходимость ряда:** ряд $\sum_{n=1}^{\infty} a_n$ сходится, если существует конечный предел его частичных сумм:
$$ \lim_{n \to \infty} S_n = S $$

Если такой предел существует и конечен, то $S$ называется **суммой ряда**.

**Расходимость ряда:** если предел $S_n$ не существует или бесконечен, ряд называется расходящимся.

### Необходимый признак сходимости ряда

Если ряд $\sum_{n=1}^{\infty} a_n$ сходится, то:
$$ \lim_{n \to \infty} a_n = 0 $$
Обратное утверждение неверно. Пример: гармонический ряд $\sum_{n=1}^{\infty} \frac{1}{n}$ расходится, хотя $\lim_{n \to \infty} \frac{1}{n} = 0$.

### Признаки сравнения рядов

#### Признак сравнения

Пусть даны два ряда $\sum_{n=1}^{\infty} a_n$ и $\sum_{n=1}^{\infty} b_n$ с положительными членами.
1. Если существует $C > 0$, такое что для всех $n$ выполнено $0 \leq a_n \leq C \cdot b_n$, и ряд $\sum_{n=1}^{\infty} b_n$ сходится, то сходится и ряд $\sum_{n=1}^{\infty} a_n$.
2. Если существует $c > 0$, такое что для всех $n$ выполнено $0 \leq c \cdot a_n \leq b_n$, и ряд $\sum_{n=1}^{\infty} b_n$ расходится, то расходится и ряд $\sum_{n=1}^{\infty} a_n$.

#### Признак предельного сравнения

Пусть даны два ряда $\sum_{n=1}^{\infty} a_n$ и $\sum_{n=1}^{\infty} b_n$ с положительными членами. Если существует конечный и положительный предел
$$ \lim_{n \to \infty} \frac{a_n}{b_n} = L, \quad 0 < L < \infty $$
то ряды $\sum_{n=1}^{\infty} a_n$ и $\sum_{n=1}^{\infty} b_n$ ведут себя одинаково: либо оба ряда сходятся, либо оба ряда расходятся.

### Признак Даламбера

Если для ряда $\sum_{n=1}^{\infty} a_n$ с положительными членами существует предел:
$$ \lim_{n \to \infty} \left| \frac{a_{n+1}}{a_n} \right| = L $$
то ряд:
- сходится при $L < 1$,
- расходится при $L > 1$,
- метод не применим при $L = 1$.

### Признак Коши

Если для ряда $\sum_{n=1}^{\infty} a_n$ с положительными членами существует предел:
$$ \lim_{n \to \infty} \sqrt[n]{|a_n|} = L $$
то ряд:
- сходится при $L < 1$,
- расходится при $L > 1$,
- метод не применим при $L = 1$.

### Интегральный признак сходимости

Рассмотрим ряд $\sum_{n=1}^{\infty} a_n$ с положительными членами и сопоставим ему функцию $f(x)$ такую, что $f(n) = a_n$ для всех целых $n$ и $f(x)$ убывающая.

Если интеграл
$$ \int_{1}^{\infty} f(x) \, dx $$
сходится, то и ряд $\sum_{n=1}^{\infty} a_n$ сходится. Если интеграл расходится, то и ряд расходится.

### Объяснения на понятном языке

- **Сходимость ряда:** это когда сумма его членов ведет себя стабильно и приближается к какому-то числу.
- **Необходимый признак сходимости:** если ряд сходится, его члены должны становиться все меньше и меньше и стремиться к нулю.
- **Признаки сравнения:** мы сравниваем наш ряд с другим, уже известным нам по поведению. Если один из них ведет себя определенным образом (сходится/расходится), то второй будет вести себя аналогично.
- **Признак Даламбера:** мы смотрим, как отношение соседних членов ряда ведет себя при $n \to \infty$. Это помогает понять, устремляется ли ряд к одному числу или нет.
- **Признак Коши:** здесь важно, как $n$-ный корень из членов ряда ведет себя при $n \to \infty$. Это еще один способ оценить сходимость.
- **Интегральный признак:** рассматривается аналогичная функция и ее неограниченный интеграл, который помогает определить поведение ряда.

Такой разделенный и подробный подход позволяет лучше понять различные методы и признаки сходимости для числовых рядов.

---

## Конспект по Теме: Проверка гипотез в статистике

### Проверка гипотез о числовых значениях параметров

#### Формулировка гипотез

1. **Нулевая гипотеза ($H_0$)**: Гипотеза, которую мы хотим проверить. Обычно это гипотеза о том, что параметр равен некоторому значению.
2. **Альтернативная гипотеза ($H_1$ или $H_a$)**: Гипотеза, противоречащая нулевой гипотезе. Например, $H_1$ может утверждать, что параметр больше, меньше или не равен предполагаемому значению.

$$
\begin{cases}
H_0: \theta = \theta_0 \\
H_1: \theta \neq \theta_0 \\
\text{или} \\
H_1: \theta > \theta_0 \\
\text{или} \\
H_1: \theta < \theta_0
\end{cases}
$$

#### Статистический критерий

- **Статистический критерий**: Правило или процедура, позволяющая решить, отклонить нулевую гипотезу или нет.
- **Критическая область (Критерий отвергания)**: Область значений статистики теста, при попадании в которую мы отклоняем $H_0$.

#### Примеры критериев

1. **Одновыборочный t-тест**: 
   Проверяет гипотезу о среднем значении в генеральной совокупности.

$$
t = \frac{\bar{x} - \mu_0}{s / \sqrt{n}}
$$
где $\bar{x}$ – выборочное среднее, $\mu_0$ – предполагаемое значение параметра, $s$ – выборочное стандартное отклонение, $n$ – размер выборки.

2. **Z-критерий для среднего**:
   Применяется для больших выборок ($n > 30$).

$$
z = \frac{\bar{x} - \mu_0}{\sigma / \sqrt{n}}
$$
где $\sigma$ – стандартное отклонение в генеральной совокупности.

### Проверка гипотез о законе распределения

#### Формулировка гипотез

- **$H_0$**: Данные распределены по конкретному закону (например, нормальному).
- **$H_1$**: Данные не распределены по этому закону.

#### Примеры критериев

1. **Критерий Хи-квадрат ($\chi^2$)**:
   Используется для проверки соответствия наблюдаемого распределения теоретическому.

$$
\chi^2 = \sum_{i=1}^k \frac{(O_i - E_i)^2}{E_i}
$$
где $O_i$ – наблюдаемые частоты, $E_i$ – ожидаемые частоты.

2. **Критерий Колмогорова-Смирнова**:
   Проверяет согласие распределения наблюдаемых данных с предполагаемым.

$$
D = \sup_x |F_n(x) - F(x)|
$$
где $F_n(x)$ – эмпирическая функция распределения, $F(x)$ – теоретическая функция распределения.

### Проверка гипотез об однородности выборок

#### Формулировка гипотез

- **$H_0$**: Обе выборки взяты из одной и той же генеральной совокупности.
- **$H_1$**: Выборки взяты из разных генеральных совокупностей.

#### Примеры критериев

1. **Двухвыборочный t-тест для независимых выборок**:
   Проверяет равенство средних двух выборок.

$$
t = \frac{\bar{x}_1 - \bar{x}_2}{\sqrt{s_p^2 \left(\frac{1}{n_1} + \frac{1}{n_2}\right)}}
$$
где $s_p^2$ – комбинированная выборочная дисперсия.

2. **U-критерий Манна-Уитни**:
   Непараметрический тест для проверки того, что медианы двух выборок равны.

### Другие задачи проверки статистических гипотез

#### Однофакторный дисперсионный анализ (ANOVA)

Проверяет равенство средних более чем двух групп.

- **Гипотезы**:
  $$
  \begin{cases}
  H_0: \mu_1 = \mu_2 = \ldots = \mu_k \\
  H_1: \exists i, j \, \text{такие что} \, \mu_i \neq \mu_j
  \end{cases}
  $$
  
- **Статистика теста**:
  
  $$
  F = \frac{\text{МС}_\text{между}}{\text{МС}_\text{внутри}}
  $$

здесь $\text{МС}_\text{между}$ и $\text{МС}_\text{внутри}$ - средние квадраты между группами и внутри групп соответственно.

#### Задачи и критерии

- **Критерий знаков**: Проверяет гипотезу о центре распределения.
- **Критерий ранговых сумм Вилкоксона**: Непараметрический тест для парных сравнений.
- **Критерий Лиллефорса**: Проверяет нормальность распределения.

---

### Объяснения

- **Нулевая и альтернативная гипотезы**: Формулировка гипотезы – важный этап. Нулевая гипотеза обычно представляет статус-кво или отсутствие эффекта, а альтернативная – то, что мы хотим доказать.
  
- **Выбор статистического критерия**: Критерий выбирается в зависимости от типа данных и проверяемой гипотезы. Например, t-тест используется для проверки гипотезы о среднем значении, а критерий Хи-квадрат для соответствия распределения.

- **Критическая область и уровень значимости ($\alpha$)**: Уровень значимости выбирается заранее (обычно 0.05 или 0.01). Это вероятность ошибочно отвергнуть нулевую гипотезу, когда она верна. Критическая область определяется таким образом, чтобы суммарная вероятность попасть в нее при верной $H_0$ была равна $\alpha$.